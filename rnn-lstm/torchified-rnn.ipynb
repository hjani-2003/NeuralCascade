{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted and modified from https://www.kaggle.com/code/fareselmenshawii/rnn-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "        \n",
    "        self.chars = list(set(data))\n",
    "        \n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "        \n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    " \n",
    "    def generate_example(self, idx):\n",
    "        \n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "        \n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "        \n",
    "        return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, hidden_size, data_generator, sequence_length, learning_rate):\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.data_generator = data_generator\n",
    "        self.vocab_size = self.data_generator.vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = torch.empty(hidden_size, self.vocab_size)\n",
    "        torch.nn.init.uniform_(self.Wax, -torch.sqrt(torch.tensor(1. / self.vocab_size)), torch.sqrt(torch.tensor(1. / self.vocab_size)))\n",
    "\n",
    "\n",
    "        self.Waa = torch.empty(hidden_size, hidden_size)\n",
    "        torch.nn.init.uniform_(self.Waa, -torch.sqrt(torch.tensor(1. / self.vocab_size)), torch.sqrt(torch.tensor(1. / self.vocab_size)))\n",
    "\n",
    "\n",
    "        self.Wya = torch.empty(self.vocab_size, hidden_size)\n",
    "        torch.nn.init.uniform_(self.Waa, -torch.sqrt(torch.tensor(1. / self.vocab_size)), torch.sqrt(torch.tensor(1. / self.vocab_size)))\n",
    "\n",
    "        self.ba = torch.zeros((hidden_size, 1))  \n",
    "        self.by = torch.zeros((self.vocab_size, 1))\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = torch.zeros_like(self.Wax), torch.zeros_like(self.Waa), torch.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = torch.zeros_like(self.ba), torch.zeros_like(self.by)\n",
    "\n",
    "        # parameter update with AdamW\n",
    "        self.mWax = torch.zeros_like(self.Wax)\n",
    "        self.vWax = torch.zeros_like(self.Wax)\n",
    "        self.mWaa = torch.zeros_like(self.Waa)\n",
    "        self.vWaa = torch.zeros_like(self.Waa)\n",
    "        self.mWya = torch.zeros_like(self.Wya)\n",
    "        self.vWya = torch.zeros_like(self.Wya)\n",
    "        self.mba = torch.zeros_like(self.ba)\n",
    "        self.vba = torch.zeros_like(self.ba)\n",
    "        self.mby = torch.zeros_like(self.by)\n",
    "        self.vby = torch.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x = x - torch.max(x)\n",
    "        p = torch.exp(x)\n",
    "        return p / torch.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "        self.X = X\n",
    "        a[-1] = a_prev.clone()\n",
    "        for t in range(len(self.X)): \n",
    "            x[t] = torch.zeros((self.vocab_size,1)) \n",
    "            if (self.X[t] != None):\n",
    "                x[t][self.X[t]] = 1\n",
    "            a[t] = torch.tanh((self.Wax @ x[t]) + (self.Waa @ a[t - 1]) + self.ba)\n",
    "            y_pred[t] = F.softmax(self.Wya @ a[t] + self.by, dim=0)\n",
    "        return x, a, y_pred \n",
    "\n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        da_next = torch.zeros_like(a[0])\n",
    "        for t in reversed(range(len(self.X))):\n",
    "            dy_preds = y_preds[t].clone()\n",
    "            dy_preds[targets[t]] -= 1\n",
    "            da = (self.Waa.T @ da_next) + (self.Wya.T @ dy_preds)\n",
    "            dtanh = (1 - torch.pow(a[t], 2))\n",
    "            da_unactivated = dtanh * da\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += (da_unactivated @ x[t].T)\n",
    "            self.dWaa += (da_unactivated @ a[t - 1].T)\n",
    "            da_next = da_unactivated\n",
    "            self.dWya += (dy_preds @ a[t].T)\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                torch.clamp(grad, -1, 1, out=grad)\n",
    " \n",
    "    def loss(self, y_preds, targets):\n",
    "        return -torch.sum(torch.stack([torch.log(torch.clamp(y_preds[t][targets[t], 0], min=1e-8)) for t in range(len(self.X))]))\n",
    "\n",
    "\n",
    "    \n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * torch.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (torch.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * torch.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat /  (torch.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * torch.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat /  (torch.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * torch.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat /  (torch.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * torch.square(self.dby)\n",
    "        # self.by -= self.learning_rate * (self.mby / (torch.sqrt(self.vby) + epsilon) + L2_reg * self.by)\n",
    "    \n",
    "    def sample(self):\n",
    "        x = torch.zeros((self.vocab_size, 1))\n",
    "        a_prev = torch.zeros((self.hidden_size, 1))\n",
    "        indices = []\n",
    "        idx = -1\n",
    "        counter = 0\n",
    "        max_chars = 50 # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n'] # the newline character\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # compute the hidden state\n",
    "            a = torch.tanh((self.Wax @ x) + (self.Waa @ a_prev) + self.ba)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y= F.softmax((self.Wya @ a) + self.by, dim=0)\n",
    "\n",
    "            # Sample an index using multinomial distribution\n",
    "            idx = torch.multinomial(y.squeeze(), num_samples=1).item()\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = torch.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # store the sampled character index in the list\n",
    "            indices.append(idx)\n",
    "\n",
    "            # update the previous hidden state\n",
    "            a_prev = a\n",
    "\n",
    "            # increment the counter\n",
    "            counter += 1\n",
    "\n",
    "        # return the list of sampled character indices\n",
    "        return indices\n",
    "\n",
    "        \n",
    "    def train(self, generated_names=5):\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 # stopping criterion for training\n",
    "        smooth_loss =  -torch.log(torch.tensor(1.0) / self.data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        # print('1. smooth: ',smooth_loss, ', threshold: ', threshold)\n",
    "        while (smooth_loss > threshold):\n",
    "            # print('2. smooth: ',smooth_loss, ', threshold: ', threshold)\n",
    "            a_prev = torch.zeros((self.hidden_size, 1))\n",
    "            idx = iter_num % self.vocab_size\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = self.data_generator.generate_example(idx)\n",
    "\n",
    "            # forward pass\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "            # print progress every 500 iterations\n",
    "            if iter_num % 500 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                for i in range(generated_names):\n",
    "                    sample_idx = self.sample()\n",
    "                    txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                    txt = txt.title()  # capitalize first character \n",
    "                    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "            # print('3. smooth: ',smooth_loss, ', threshold: ', threshold)\n",
    "    \n",
    "    def predict(self, start):\n",
    "        # Initialize input vector and previous hidden state\n",
    "        x = torch.zeros((self.vocab_size, 1))\n",
    "        a_prev = torch.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Convert start sequence to indices\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = self.data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # Generate sequence\n",
    "        max_chars = 50  # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n']  # the newline character\n",
    "        counter = 0\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # Compute next hidden state and predicted character\n",
    "            a = torch.tanh((self.Wax @ x) + (self.Waa @ a_prev) + self.ba)\n",
    "            # Compute probabilities\n",
    "            y_pred = F.softmax((self.Wya @ a) + self.by, dim=0)\n",
    "\n",
    "            # Sample an index using multinomial distribution\n",
    "            idx = torch.multinomial(y_pred.squeeze(), num_samples=1).item()\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = torch.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            a_prev = a\n",
    "            idxes.append(idx)\n",
    "            counter += 1\n",
    "\n",
    "        # Convert indices to characters and concatenate into a string\n",
    "        txt = ''.join(self.data_generator.idx_to_char[i] for i in idxes)\n",
    "\n",
    "        # Remove newline character if it exists at the end of the generated sequence\n",
    "        if txt[-1] == '\\n':\n",
    "            txt = txt[:-1]\n",
    "\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:82.330002\n",
      "\n",
      "Wxsvppfvqrayromvqysvmbnaszysk\n",
      "Rwoplqchlwjjxwdl\n",
      "Qpaiazqpeziuamk\n",
      "Jbxakvr\n",
      "Ynsrsxhbztlfbdcxqxaomxuh\n",
      "\n",
      "\n",
      "iter :500, loss:56.124008\n",
      "\n",
      "Illaot\n",
      "Miaoa\n",
      "Lino\n",
      "Er\n",
      "Tloae\n",
      "\n",
      "\n",
      "iter :1000, loss:39.146660\n",
      "\n",
      "Miryineh\n",
      "Elaraoe\n",
      "Oia\n",
      "Diee\n",
      "Avioort\n",
      "\n",
      "\n",
      "iter :1500, loss:28.470366\n",
      "\n",
      "Ludaayh\n",
      "Vinoaelh\n",
      "Pecl\n",
      "Char\n",
      "Rpl\n",
      "\n",
      "\n",
      "iter :2000, loss:21.730610\n",
      "\n",
      "Bill\n",
      "Ulaeitl\n",
      "Yiil\n",
      "Melloie\n",
      "Olleri\n",
      "\n",
      "\n",
      "iter :2500, loss:17.291380\n",
      "\n",
      "Caaae\n",
      "Ml\n",
      "Laaall\n",
      "Naa\n",
      "Star\n",
      "\n",
      "\n",
      "iter :3000, loss:14.300630\n",
      "\n",
      "Elig\n",
      "Vinl\n",
      "Virl\n",
      "Lu\n",
      "Olarai\n",
      "\n",
      "\n",
      "iter :3500, loss:12.255441\n",
      "\n",
      "Eleberla\n",
      "Fegl\n",
      "Peg\n",
      "Veaali\n",
      "Avatl\n",
      "\n",
      "\n",
      "iter :4000, loss:10.799430\n",
      "\n",
      "Eliior\n",
      "Peaal\n",
      "Hiayi\n",
      "Pea\n",
      "Abery\n",
      "\n",
      "\n",
      "iter :4500, loss:9.701606\n",
      "\n",
      "Hare\n",
      "Rlaraa\n",
      "Mia\n",
      "Ziayi\n",
      "Scarya\n",
      "\n",
      "\n",
      "iter :5000, loss:8.874021\n",
      "\n",
      "Mia\n",
      "Abe\n",
      "Ha\n",
      "Via\n",
      "Mia\n",
      "\n",
      "\n",
      "iter :5500, loss:8.258652\n",
      "\n",
      "Laa\n",
      "Olarla\n",
      "Mia\n",
      "Aberla\n",
      "Mily\n",
      "\n",
      "\n",
      "iter :6000, loss:7.698971\n",
      "\n",
      "Scaria\n",
      "Hartort\n",
      "Abirla\n",
      "Viia\n",
      "Hapilepe\n",
      "\n",
      "\n",
      "iter :6500, loss:7.260252\n",
      "\n",
      "Isarlet\n",
      "Giatori\n",
      "Abe\n",
      "Pelre\n",
      "Ele\n",
      "\n",
      "\n",
      "iter :7000, loss:6.899681\n",
      "\n",
      "Pea\n",
      "Beia\n",
      "Abebia\n",
      "Abi\n",
      "Mia\n",
      "\n",
      "\n",
      "iter :7500, loss:6.571926\n",
      "\n",
      "Haria\n",
      "Laa\n",
      "Miva\n",
      "Ria\n",
      "Mia\n",
      "\n",
      "\n",
      "iter :8000, loss:6.247831\n",
      "\n",
      "Abi\n",
      "Ola\n",
      "Eliaail\n",
      "\n",
      "Una\n",
      "\n",
      "\n",
      "iter :8500, loss:6.092215\n",
      "\n",
      "Ria\n",
      "Rca\n",
      "Sca\n",
      "Elllyn\n",
      "Haria\n",
      "\n",
      "\n",
      "iter :9000, loss:5.941021\n",
      "\n",
      "Laia\n",
      "Oli\n",
      "Abigie\n",
      "Isar\n",
      "Pena\n",
      "\n",
      "\n",
      "iter :9500, loss:5.792557\n",
      "\n",
      "Olia\n",
      "Lepa\n",
      "Lariytiha\n",
      "Oli\n",
      "Bela\n",
      "\n",
      "\n",
      "iter :10000, loss:5.670740\n",
      "\n",
      "Abelaop\n",
      "Chna\n",
      "Vada\n",
      "Lapea\n",
      "Oze\n",
      "\n",
      "\n",
      "iter :10500, loss:5.525096\n",
      "\n",
      "Perye\n",
      "Vila\n",
      "Gralapia\n",
      "Chna\n",
      "Viga\n",
      "\n",
      "\n",
      "iter :11000, loss:5.371145\n",
      "\n",
      "Pelae\n",
      "Ylie\n",
      "Beria\n",
      "Zarle\n",
      "Sca\n",
      "\n",
      "\n",
      "iter :11500, loss:5.242846\n",
      "\n",
      "Elegaiia\n",
      "Lunelope\n",
      "Ola\n",
      "Mila\n",
      "\n",
      "\n",
      "\n",
      "iter :12000, loss:5.165024\n",
      "\n",
      "Filia\n",
      "Mila\n",
      "Oliblett\n",
      "Mila\n",
      "Damia\n",
      "\n",
      "\n",
      "iter :12500, loss:5.091405\n",
      "\n",
      "Isarlope\n",
      "Laraa\n",
      "Zarpeta\n",
      "Sofia\n",
      "Scarlette\n",
      "\n",
      "\n",
      "iter :13000, loss:5.049176\n",
      "\n",
      "Graison\n",
      "Via\n",
      "Ismil\n",
      "Grator\n",
      "Larpettte\n"
     ]
    }
   ],
   "source": [
    "data_generator = DataGenerator('names.txt')\n",
    "rnn = RNN(hidden_size=200,data_generator=data_generator, sequence_length=25, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'putrahe'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"putra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'za'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(\"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
